\documentclass{beamer}
\usetheme{Boadilla}
%\usecolortheme{fly}


\title{Comp 6321 - Machine Learning}
\subtitle{Using Neural Nets for playing othello}
\author{Federico O'Reilly Regueiro}
\institute{Concordia University}
\date{\today}


\begin{document}

\begin{frame}
\titlepage
\end{frame}

\setbeamercovered{transparent}

%------------- Statement --------------------
\section{Problem Statement}
\begin{frame}
\frametitle{Problem statement}

\begin{itemize}
\item<1->Zero-sum, perfect-knowledge (no chance involved) competitive-game\\
\begin{itemize}
\item<2,3,4>A sandbox - toy-representation of reality
\item<3,4>bounded problem space with clear goal and set of rules
\item<4>bounded, but can be huge (ie, GO - $10^{761}$ possible games!) \cite{AlphaGo}
\end{itemize}
\item<5->Can a machine learn to play 	
\begin{itemize}
\item<6,7,8>One of the oldest questions in AI
\item<7,8>The trick is in finding ways to narrow the search   
\item<8>Has been well answered, requiring less expert knowledge each  time
\end{itemize}
\item<9->Without expert knowledge (rules or labels)...
\begin{itemize}
\item<10>...the feedback becomes very sparse
\end{itemize}
\end{itemize}
\end{frame}


\section{Approaches and solutions}

\begin{frame}
\frametitle{Approaches and solutions - common elements}
\begin{itemize}
\item<1-> Classification problem
\begin{itemize}
\item<2,4> Dual class\\ \pause given a game-state, what are the odds of winning 
\begin{itemize}
\item<4> Look ahead n-moves - (n-ply) - then decide best path given leaf `value'
\end{itemize}
\item<3,5> Multi-class \\ \pause given a game-state, what is the best next move
\begin{itemize}
\item<5-> Learn a `policy' for action given a state $P(a | s)$
\end{itemize}
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Approaches and solutions to the problem}
\begin{itemize}
\item<1->Rule-based approach dependent on expert knowledge
\begin{itemize}
\item<2>e.g. Deep Blue
\end{itemize}
\item<3->Supervised learning - collect labeled states and train
\begin{itemize}
\item<4,5>Also depends on human expert knowledge
\item<5>Labor intensive collection and labeling
\end{itemize}
\item<6->Genetic optimizations - Evolutionary NNs
\begin{itemize}
\item<7,8>Does not exploit NNs learning capabilities\\ but won't get stuck on local minima...
\item<8>Slow to converge
\item<9>Capable of finding innovative strategies \cite{Moriarty93evolvingcomplex} \cite{chellapilla1999evolution}
\end{itemize}
\item<10->Reinforcement learning
\begin{itemize}
\item<11,12,13,14,15>TD-learning 
	\begin{itemize}
	\item<12>Tesauro's TD-Backgammon - chance element
	\end{itemize}
\item<13,14,15>Like having sparse and time-delayed labels
\item<14,15>Credit assignment problem
\item<15>explore-exploit dilemma
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Current state-of-the-art}
\begin{frame}
\frametitle{Current state-of-the-art}
\begin{itemize}
\item<1->Alpha-go
\begin{itemize}
\item<2,3>Two policy convolutional networks - 1 large, 1  small - prune search tree\\$TD(\lambda)$
\item<3>One Fully connected - predict win\\value
\end{itemize}
\item<4->DeepMind Atari deep reinforcement learning
\begin{itemize}
\item<5>Deep neural nets meet $TD(\lambda)$
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Current project state}
\begin{frame}
\frametitle{Current project state - discarded approaches}
\begin{itemize}
\item<1->Supervised Learning
\begin{itemize}
\item<2>Acquiring sets is a cumbersome task\\ requires an overhead outside of ML - eg Edax
\end{itemize}
\item<3->Rule-based
\begin{itemize}
\item<4>Heuristic - Decision tree - in place but focus is on nets
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Current project state - narrowed approaches}
\begin{itemize}
\item<1->TD learning
\begin{itemize}
\item<2,3>Similar to back propagation but recurses temporally
\item<3>$\Delta_{w_t} = \alpha(P_{t+1} - P_t)\sum_{k=1}^{t} \lambda^{t-k}\nabla_w P_k  ,\quad 0 \leq \lambda \leq 1 $
\item<4->Based on Leouski and Utgoff's paper\cite{Leouski96whata}
\item<5>They use symmetry, rotation and weight sharing - 96 h.u.\\ - turn into conv net
\end{itemize}
\item<6->ENN
\begin{itemize}
\item<7-11>Based on Chelapilla and Fogel\cite{chellapilla1999evolution}
\item<8-10>Generation has 15 strategies, change vector $\sigma_i(j)$ for $j^{th}$ weight of $i^{th}$ strategy.
\item<9>$\sigma'_i(j) = \sigma_i(j) exp(\tau N_j(0,1))$
\item<10>$w'_i(j) = w_i(j) + \sigma_i(j)N_j(0,1)$
\end{itemize}
\end{itemize}
\end{frame}

\section{References}

\begin{frame}%[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{plain} % amsalpha for shorter layout apalike for longer?
        \bibliography{../ML-AI-proj-2016}
\end{frame}
\end{document}