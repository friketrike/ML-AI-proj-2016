\documentclass{article}

% \usepackage[margin=0.75in]{geometry}

\usepackage{titlesec}
\usepackage{cite}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Comp 6321 - Machine Learning - Project report}
\author{Federico O'Reilly Regueiro}
\date{December 15$^{th}$, 2016}
\maketitle

%An introduction/motivation section, in which you explain why the problem you are about to address is interesting and challenging
%A section describing the basic approach that you will take. You should assume that algorithms we discuss in class (e.g. SVMs, HMMs etc.) are known, but summarize any algorithms that would not have been discussed in class (e.g. conditional random fields, multi-dimensional scaling etc.)
%A section describing the experimental setup. Here you describe your data set, along with any pre-processing steps that you might have taken (e.g. to remove noise, select attributes etc.). You should describe this in enough detail that someone with access to your data could exactly reproduce the results
%A section describing your results, along with a discussion of what you observed. It is important to ensure that you perform your experiments in such a way that results are meaningful (e.g., make sure you use cross-validation and report test set results). If you use statistical testing to assess the significance of your results, make sure that the test you choose is appropriate for your data. If appropriate, report running time in addition to performance.
%A section containing conclusions and possible future work directions.
%A section of references
%An Appendix in PPT format in a form of a short tutorial surveying the subject of the project.  

%------------------------ Introduction ------------------------%
\section{Introduction and motivation}
The current proliferation of readily available data collections that has been spawned by the inetrnet
has set the stage for machine learning as a pervasive means to develop different kinds of intelligent systems.

It is, however, the heterogeneity and inconsistency of such data that limits what can be done by 
regular supervised learning. Some of the biggest efforts put forth while tackling a supervised learning problem continue
to be collection and curation of data sets. 

Additionally, the notion of labeling all or most input instances becomes unwieldy under certain circumstances.

The lack of clear or constant labels to inputs gives rise to two forms of learning that are not supervised.
Unsupervised learning, where algorithms aim at finding underlying structure in the data and reinforcement learning; which
is in some ways between supervised learning and unsupervised learning, where 'labels' are sparse and time-delayed\cite{demistyfyingRL}. 
This same sparsity and delay give rise to what is referred to as 'the credit assignment problem', where given a certain outcome from a 
series of actions, it is difficult to ascertain which, if any, of the actions leading to the outcome bears the largest responsibility
for the outcome.

There are several approaches to reinforcement learning, we focus on a particular type which tries to assign the credit of a given outcome
somewhat evenly amonge the events leading to it. It does so by supposing that at any point in time the reward (or penalty) is equal to
the reward gained at that point plus the sum of possible discounted future rewards. The rationale for discounting rewards over time 
corresponds to the notion of rewards being more desirable now than later on\footnote{As Andrew Ng puts it in his online lecture
on the topic, we might be dead tomorrow.\cite{AngRLlecture}}. This idea, central to temporal difference learning will be presented more
fully in section \ref{whatSection}.% TODO define section

Applications of reinforcement learning are varied and curretly under development in fields such as vehicle control, robotics,
prediction of streaming data such as that of financial applications and gaming to name a few.

As Moriarty and Miikkulainen put it, 'games are an important domain for studying problem-solving strategies.'\cite{Moriarty93evolvingcomplex}
Traditionally games, due to their well-defined rules, state-transitions and goal, have made a good sandbox for the development of
any form of intelligent agents.

Much has been achieved in this field, recently Google's Alpha-go bested top-ranking go-player Lee Sedol. Go had traditionally been
seen as the unattainable goal in cmoputer game-playing given the vastness of it's state-space. Alpha-go achieved this outstanding 
result with the combination of different strategies, via 3 Neural Networks\cite{AlphaGo}. Although there are several formulations,
traditionally there have been two main approaches to temporal difference:
to temporal difference learning

%------------------------ Description ------------------------%
\section{Description of approach}
-Many forms of TD learning (cite paper)\\

-What we do, see the ideal sequence of time-discounted predictions as a smooth progression (*Think of a more mathematically friendly term)
---describe the td error equation (requires reading Tesauro a bit more)\\

-We use a convolutional net and a fc net (sort of justification)
---Justification and description of convolutional nets are intermingled---justify the lack of pooling (look for citation)\\

Other approaches and why we don' t use them
---Q-learning, currently mentioned often. Difference between Value expansion and q or policy expansion and implications for the problem
---Older - ENN - interesting but requires starting generations very much in advance\\

%------------------------ Setup ------------------------%
\section{Setup}
-using tensorflow, w / python 3.5 api and numpy
---how tensorflow works, still getting the hang of it\\

-Brief description of the board model as an input matrix
-Description of the network (Latex NN package?)

--Avoue qu' il y a knowledge representation - we avoided the symmetry proposed by Ugosky(cite) given what he describes as a problem but
expanded the idea

--Important to mention: output is squashed token ratio\\

-Routine for training / testing - given time\\

Justification of training against a random player, brief mention of next steps
---Exploration of the space (exploration/exploitation)
--question: is playing black generally a disadvantage? nets tend to start with more losses than wins
--question: is playing whites (times -1) much different\\

%------------------------ Results ------------------------%
\section{Results}
-Expected- evaluations of the board should move towards 0 at the onset and relatively smoothly progress towards the end result\\

Problems constraining the obtained results - memory leak, resulting lack of time due to long training\\

No comparison of different lambda, gamma, lr dropout values -- many hyper-params to tune!\\

convergence? How does it stack up. How many epochs, plots of wins\\

plots of token ratios per epoch



%----------------- from future import work -------------------%
\section{Future work}
Compare initialization vs randomPlayer with different parameters

Initialize several nets against randomPlayer

pit them against each other for training

they should all develop unique strategies so then they help each other out\\

Maybe combining training with random player and then Gen Algo approach?






%------------------------------------------- Bibliography ----------------------------------------------------
\bibliography{ML-AI-proj-2016-proposal}
\bibliographystyle{plain}
\end{document}
